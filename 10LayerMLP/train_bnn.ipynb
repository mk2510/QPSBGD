{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "### fixed\n",
    "#from libsvm.svmutil import *\n",
    "from libsvm.svmutil import svm_read_problem\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import struct\n",
    "import networkx as nx\n",
    "import dwave_networkx as dnx\n",
    "import matplotlib.pyplot as plt\n",
    "import dimod\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, FixedEmbeddingComposite\n",
    "from dwave.cloud import Client\n",
    "from dwave.system import DWaveSampler\n",
    "from dwave.embedding import EmbeddedStructure\n",
    "import dwave.inspector\n",
    "import dwave.embedding\n",
    "from minorminer import find_embedding\n",
    "import itertools\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "\n",
    "from qubovert import PCBO, boolean_var\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#! wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\n",
    "#! wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "# #\n",
    "# #\n",
    "# Leap_API_token = '' ### from the leap account dashboard at https://cloud.dwavesys.com/\n",
    "# DATA = '/home/sasde/Downloads/'\n",
    "\n",
    "# client = Client.from_config(token=Leap_API_token)\n",
    "# solvers = client.get_solvers(num_qubits__gt=3000)\n",
    "# solver = solvers[0]\n",
    "# solver\n",
    "# G = nx.Graph()\n",
    "# G.add_edges_from( solver.edges )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as grb\n",
    "from gurobipy import GRB\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = './datafolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_up_data(sel=3):\n",
    "    \n",
    "    ## selects a subset of features from the a1a dataset\n",
    "    \n",
    "    #global x\n",
    "    global y\n",
    "    #global x_test\n",
    "    global y_test\n",
    "    global select\n",
    "    global x_data_reduced\n",
    "    global x_test_data_reduced\n",
    "    y, x = svm_read_problem(DATA +'/a1a')\n",
    "    y_test, x_test = svm_read_problem(DATA +'/a1a.t')\n",
    "    np.random.seed(2)\n",
    "    train_indexes = list(range(len(y)))\n",
    "    np.random.shuffle(train_indexes)\n",
    "    y = [y[i] for i in train_indexes]\n",
    "    x = [x[i] for i in train_indexes]\n",
    "    x_data, exists = data_to_matrix(x)\n",
    "    x_test_data, exists_test = data_to_matrix(x_test)\n",
    "\n",
    "    if sel== 0.3:\n",
    "        select = np.mean(x_data,0) > .3\n",
    "        np.array(range(123))[select]\n",
    "    elif sel== 7:     ### selects 7 features\n",
    "        select = (((np.mean(x_data,0) > .3) + 0.) + (np.mean(x_data,0) < .65)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "    elif sel== 9:      ### selects 9 features\n",
    "        select = (((np.mean(x_data,0) > .3) + 0.) + (np.mean(x_data,0) < .7)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "\n",
    "    elif sel== 10:\n",
    "        select = (((np.mean(x_data,0) > .25) + 0.) + (np.mean(x_data,0) < .74)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "\n",
    "    elif sel== 15:      ### selects 15 features\n",
    "        select = (((np.mean(x_data,0) > .21) + 0.) + (np.mean(x_data,0) < .79)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "    elif sel == 5:\n",
    "        select = (((np.mean(x_data,0) > .34) + 0.) + (np.mean(x_data,0) < .65)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "    elif sel == 4:\n",
    "        select = (((np.mean(x_data,0) > .34) + 0.) + (np.mean(x_data,0) < .46)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "    elif sel == 6:\n",
    "        select = (((np.mean(x_data,0) > .33) + 0.) + (np.mean(x_data,0) < .65)) == 2.\n",
    "        np.array(range(123))[select]\n",
    "\n",
    "\n",
    "    elif sel == 2:      ### selects 2 features\n",
    "        select = np.array([False for i in range(123)])\n",
    "        select[6] = True\n",
    "        select[22] = True\n",
    "        select\n",
    "    elif sel == 1:      ### selects 1 features\n",
    "        select = np.array([False for i in range(123)])\n",
    "        select[22] = True\n",
    "        select\n",
    "        \n",
    "    elif sel == 3:      ### selects 3 features\n",
    "        select = np.array([False for i in range(123)])\n",
    "        select[6] = True\n",
    "        select[22] = True\n",
    "        select[36] = True\n",
    "        select\n",
    "    elif sel== 'all':\n",
    "        select = np.array([True for i in range(123)])\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    x_data_reduced = x_data[:,select]\n",
    "    x_test_data_reduced = x_test_data[:,select]\n",
    "    \n",
    "    \n",
    "    y = 0+(np.array(y) > 0.)\n",
    "    y_test = 0+(np.array(y_test) > 0.)\n",
    "\n",
    "#set_up_data(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# BNN code adapted from:\n",
    "# github.com/Akashmathwani/Binarized-Neural-networks-using-pytorch\n",
    "# \n",
    "\n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "        #output[input >= 0] = 0.9999\n",
    "        #output[input < 0] = -0.9999\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "class BinaryTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryTanh, self).__init__()\n",
    "        #self.hardtanh = nn.Hardtanh() ## during the backward pass the hardtanh is used \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1.001,max_val=1.001) ## during the backward pass the hardtanh is used \n",
    "        #self.hardtanh = nn.Tanh() ## during the backward pass the hardtanh is used \n",
    "        #self.hardtanh = nn.Identity()\n",
    "    def forward(self, input):\n",
    "        #output = self.hardtanh(input/2.1)\n",
    "        output = self.hardtanh(input)\n",
    "        output = binarize(output) ## the output is binarized {-1,1} in the forward pass\n",
    "        \n",
    "        #if num_classes == 2:     #### CHECK THIS\n",
    "        #    output += 1\n",
    "        #    output /= 2\n",
    "        #    output = output.squeeze()\n",
    "\n",
    "        #print(output)\n",
    "        return output\n",
    "        \n",
    "class BinaryTanhProx(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryTanhProx, self).__init__()\n",
    "        #self.hardtanh = nn.Hardtanh() ## during the backward pass the hardtanh is used \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1.001,max_val=1.001) ## during the backward pass the hardtanh is used \n",
    "        #self.hardtanh = nn.Tanh() ## during the backward pass the hardtanh is used \n",
    "        #self.hardtanh = nn.Identity()\n",
    "    def forward(self, input):\n",
    "        #output = self.hardtanh(input/2.1)\n",
    "        output = self.hardtanh(input)\n",
    "        \n",
    "        #if num_classes == 2:     #### CHECK THIS\n",
    "        #    output += 1\n",
    "        #    output /= 2\n",
    "        #    output = output.squeeze()\n",
    "\n",
    "        #print(output)\n",
    "        return output\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = binarize(self.weight) ## the weights are binarized {-1,1} in the forward pass\n",
    "        if self.bias is None:\n",
    "            return F.linear(input, binary_weight)\n",
    "        else:\n",
    "            return F.linear(input, binary_weight, self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv    #### this changes the lr for the weights by a constant factor\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "## Define the NN architecture\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryNet, self).__init__()\n",
    "        self.fc_list = nn.ModuleList()\n",
    "        if num_classes == 2:\n",
    "            output_net_size = 1\n",
    "        else:\n",
    "            output_net_size = num_classes\n",
    "        input_layer_size = input_size\n",
    "        output_layer_size = hidden_n\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            if i == n_layers -1:\n",
    "                output_layer_size = output_net_size\n",
    "            self.fc_list.append( BinaryLinear(input_layer_size, output_layer_size, bias=False) )\n",
    "            input_layer_size = hidden_n\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation = BinaryTanh()\n",
    "\n",
    "    def forward(self, x, return_list=None):\n",
    "        # flatten image input\n",
    "        x_ = x.view(-1, input_size)\n",
    "        # add hidden layer, with relu activation function\n",
    "        out_list = []\n",
    "        for fkey, fc_layer in enumerate(self.fc_list):\n",
    "\n",
    "            x_before = fc_layer(x_)\n",
    "            if return_list != None:\n",
    "                #assert isinstance(return_list, list)\n",
    "                if fkey in return_list or return_list[0] == 'all':\n",
    "                    x_before.retain_grad()\n",
    "                    if fkey != 0:\n",
    "                        #print(fkey)\n",
    "                        x_.retain_grad()\n",
    "                    #x.retain_grad()\n",
    "                    fc_layer.input_features = x_\n",
    "                    fc_layer.before_activation = x_before ## debug\n",
    "                    out_list.append(x_before)\n",
    "            x_ = self.activation(x_before)\n",
    "            if return_list != None:\n",
    "                #assert isinstance(return_list, list)\n",
    "                if fkey in return_list or return_list[0] == 'all':\n",
    "                    x_.retain_grad()\n",
    "                    fc_layer.output_features = x_\n",
    "                    out_list.append(x_)\n",
    "        if return_list != None:\n",
    "             return x_, out_list\n",
    "        return x_\n",
    "\n",
    "class BinaryLinearProx(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = self.weight # binarize(self.weight) ## the weights are binarized {-1,1} in the forward pass\n",
    "        if self.bias is None:\n",
    "            return F.linear(input, binary_weight)\n",
    "        else:\n",
    "            return F.linear(input, binary_weight, self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv    #### this changes the lr for the weights by a constant factor\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "## Define the NN architecture\n",
    "class BinaryNetProxQuant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryNetProxQuant, self).__init__()\n",
    "        self.fc_list = nn.ModuleList()\n",
    "        if num_classes == 2:\n",
    "            output_net_size = 1\n",
    "        else:\n",
    "            output_net_size = num_classes\n",
    "        input_layer_size = input_size\n",
    "        output_layer_size = hidden_n\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            if i == n_layers -1:\n",
    "                output_layer_size = output_net_size\n",
    "            self.fc_list.append( BinaryLinearProx(input_layer_size, output_layer_size, bias=False) )\n",
    "            input_layer_size = hidden_n\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activation = BinaryTanhProx()\n",
    "\n",
    "    def forward(self, x, return_list=None):\n",
    "        # flatten image input\n",
    "        x_ = x.view(-1, input_size)\n",
    "        # add hidden layer, with relu activation function\n",
    "        out_list = []\n",
    "        for fkey, fc_layer in enumerate(self.fc_list):\n",
    "\n",
    "            x_before = fc_layer(x_)\n",
    "            if return_list != None:\n",
    "                #assert isinstance(return_list, list)\n",
    "                if fkey in return_list or return_list[0] == 'all':\n",
    "                    x_before.retain_grad()\n",
    "                    if fkey != 0:\n",
    "                        #print(fkey)\n",
    "                        x_.retain_grad()\n",
    "                    #x.retain_grad()\n",
    "                    fc_layer.input_features = x_\n",
    "                    fc_layer.before_activation = x_before ## debug\n",
    "                    out_list.append(x_before)\n",
    "            x_ = self.activation(x_before)\n",
    "            if return_list != None:\n",
    "                #assert isinstance(return_list, list)\n",
    "                if fkey in return_list or return_list[0] == 'all':\n",
    "                    x_.retain_grad()\n",
    "                    fc_layer.output_features = x_\n",
    "                    out_list.append(x_)\n",
    "        if return_list != None:\n",
    "             return x_, out_list\n",
    "        return x_\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD or QUBO update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import *\n",
    "def set_up_and_optimize_qubo(X, Y, W, print_debug=False, epsilon_zero=0.01):\n",
    "    \n",
    "    # using standard vars and quicksum\n",
    "\n",
    "    Y_numpy = Y.numpy()\n",
    "    X_numpy = X.numpy()\n",
    "    grb_model_min_y_swx = grb.Model()\n",
    "    grb_model_min_y_swx.setParam( 'OutputFlag', False )\n",
    "\n",
    "    W_binary = [ [ grb_model_min_y_swx.addVar( vtype=GRB.BINARY) for j in range(W.shape[1]) ] for i in range(W.shape[0]) ]\n",
    "    W_spin = [ [ grb_model_min_y_swx.addVar( vtype=GRB.CONTINUOUS, lb=-2) for j in range(W.shape[1]) ] for i in range(W.shape[0]) ]\n",
    "    for i  in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            grb_model_min_y_swx.addConstr( W_spin[i][j] == W_binary[i][j]*2 -1) \n",
    "    Z_float = [ [ grb_model_min_y_swx.addVar(lb=-100) for j in range(Y.shape[1]) ] for i in range(Y.shape[0]) ]    #     shape=tuple(Y.shape), vtype=GRB.CONTINUOUS, name=\"before_activation\", lb= -100)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            # Z_float[i] = W_spin @ x_numpy[i]\n",
    "            grb_model_min_y_swx.addConstr( grb.quicksum( [ X_numpy[i][k] * W_spin[j][k] for k in range(W.shape[1])] )  == Z_float[i][j] )\n",
    "    Y_binary = [ [ grb_model_min_y_swx.addVar(vtype=GRB.BINARY) for j in range(Y.shape[1]) ] for i in range(Y.shape[0]) ]\n",
    "    Y_spin = [ [ grb_model_min_y_swx.addVar(vtype=GRB.CONTINUOUS, lb = -2) for j in range(Y.shape[1]) ] for i in range(Y.shape[0]) ]\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            grb_model_min_y_swx.addConstr( Y_spin[i][j] == Y_binary[i][j]*2 -1 )\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            #grb_model_min_y_swx.addConstr( Z_float[i][j]  * Y_spin[i][j]  >= 0 )  ## matrices 1x1\n",
    "            grb_model_min_y_swx.addConstr( (Z_float[i][j] + epsilon_zero) * Y_spin[i][j]  >= 0 )  ## matrices 1x1\n",
    "    Y_diff_ = [ [ grb_model_min_y_swx.addVar(vtype=GRB.CONTINUOUS, lb =-100) for j in range(Y.shape[1]) ] for i in range(Y.shape[0]) ]\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            grb_model_min_y_swx.addConstr( (Y_spin[i][j] - Y_numpy[i,j])  == Y_diff_[i][j] )  ## matrices 1x1\n",
    "    grb_model_min_y_swx.setObjective( grb.quicksum( [ Y_diff_[i][j] * Y_diff_[i][j]  for i in range(Y.shape[0]) for j in range(Y.shape[1]) ] ), GRB.MINIMIZE )  ## min \\\\ Y_diff_ ||_F\n",
    "\n",
    "    grb_model_min_y_swx.feasRelaxS(0, False, False, True)\n",
    "    grb_model_min_y_swx.setParam('NonConvex', 2)\n",
    "    grb_model_min_y_swx.optimize()\n",
    "\n",
    "    if print_debug:\n",
    "        print('W_spin')\n",
    "        print(np.array([[x.X for x in y] for y in W_spin] ))\n",
    "        print('Z_float')\n",
    "        print(np.array([[x.X for x in y] for y in Z_float] ))\n",
    "        #print(Z_float.X)\n",
    "        print('Y_spin')\n",
    "        print(np.array([[x.X for x in y] for y in Y_spin] ))\n",
    "        print('Y_diff_2')\n",
    "        print(np.array([[x.X for x in y] for y in Y_diff_] ))\n",
    "        #print(Y_spin.X)\n",
    "        #print(Y_diff_2.X)\n",
    "        print('Obj: %g' % grb_model_min_y_swx.objVal)\n",
    "\n",
    "    #return W_spin\n",
    "    return np.array([[x.X for x in y] for y in W_spin] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old code necessary for old implementation\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"BNN_on_QPU_libraries\", \"./BNN_on_QPU_libraries_dimod_10.py\")\n",
    "BNN_on_QPU_libraries = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(BNN_on_QPU_libraries)\n",
    "\n",
    "BNN_on_QPU_libraries.alpha = 0.\n",
    "BNN_on_QPU_libraries.Lambda_qubo = 50\n",
    "BNN_on_QPU_libraries.num_classes = 2            ### binary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN_on_QPU_libraries.USE_OLD_GREATER_ZERO = False\n",
    "BNN_on_QPU_libraries.USE_OLD_XNOR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, update_type='sgd', grad_compare=None, epoch = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    update_type='sgd'\n",
    "    \n",
    "    sgd\n",
    "    \n",
    "    update_type='qubo'\n",
    "    \n",
    "    argmin_W ( sum_i( |Y_i - sigma(W.X_i)| ) )\n",
    "    \"\"\"\n",
    "    ls = []\n",
    "    for n_l in range(n_layers):\n",
    "        W = model.fc_list[n_l].weight      \n",
    "        if update_type== 'sgd':\n",
    "            #X.data -= learning_rate*model.fc_list[n_l].weight.grad\n",
    "            with torch.no_grad():\n",
    "                #W.copy_( W.data -args.learning_rate*model.fc_list[n_l].weight.grad )\n",
    "                W.copy_( W.data -args.learning_rate*W.grad )\n",
    "\n",
    "                t = torch.sign(grad_compare[n_l]).flatten()\n",
    "                temp = torch.eq(t[t != 0],torch.sign(W.grad).flatten()[t != 0]).sum()\n",
    "                \n",
    "                # if temp.item() == 150:\n",
    "                #     print(torch.sign(W.grad))\n",
    "                #     print(W_optimized)\n",
    "                #     print('-------------')\n",
    "                # ls.append(temp.item() / np.prod(grad_compare[n_l].shape))\n",
    "                ls.append([temp.item(), (t != 0).sum().item()])\n",
    "        if update_type == 'proxquant':\n",
    "            with torch.no_grad():\n",
    "                W.copy_( W.data -args.learning_rate*W.grad)\n",
    "                # print(W.grad)\n",
    "                lam =  0.01 * (epoch + 1)\n",
    "                # W.copy_(torch.sign(W_theta + torch.sign(W_theta - torch.sign(W_theta)) * torch.nn.functional.relu(torch.abs(W_theta - torch.sign(W_theta))- lam)))\n",
    "        \n",
    "\n",
    "                p_sign, p_abs = W.sign(), W.abs()\n",
    "                a = p_sign + torch.sign(W - p_sign) * torch.relu(torch.abs(W - p_sign) - lam)\n",
    "                # (F.relu((p_abs - 1).abs() - lam) * (p_abs - 1).sign() + 1)\n",
    "                print(a)\n",
    "                W.copy_(a)\n",
    "                \n",
    "                t = torch.sign(grad_compare[n_l]).flatten()\n",
    "                temp = torch.eq(t[t != 0],torch.sign(W.grad).flatten()[t != 0]).sum()\n",
    "                \n",
    "                # if temp.item() == 150:\n",
    "                #     print(torch.sign(W.grad))\n",
    "                #     print(W_optimized)\n",
    "                #     print('-------------')\n",
    "                # ls.append(temp.item() / np.prod(grad_compare[n_l].shape))\n",
    "                ls.append([temp.item(), (t != 0).sum().item()])\n",
    "        if update_type== 'signsgd':\n",
    "            #X.data -= learning_rate*model.fc_list[n_l].weight.grad\n",
    "            with torch.no_grad():\n",
    "                #W.copy_( W.data -args.learning_rate*model.fc_list[n_l].weight.grad )\n",
    "                W.copy_( W.data - args.learning_rate*torch.sign(W.grad) )\n",
    "\n",
    "                t = torch.sign(grad_compare[n_l]).flatten()\n",
    "                temp = torch.eq(t[t != 0],torch.sign(W.grad).flatten()[t != 0]).sum()\n",
    "                \n",
    "                # if temp.item() == 150:\n",
    "                #     print(torch.sign(W.grad))\n",
    "                #     print(W_optimized)\n",
    "                #     print('-------------')\n",
    "                # ls.append(temp.item() / np.prod(grad_compare[n_l].shape))\n",
    "                ls.append([temp.item(), (t != 0).sum().item()])\n",
    "        elif update_type == 'qubo':\n",
    "            X = model.fc_list[n_l].input_features.data  /  torch.norm(model.fc_list[n_l].input_features.data, dim=0)**2\n",
    "            Y = model.fc_list[n_l].output_features.grad\n",
    "            # print(X.shape)\n",
    "            # print(Y.shape)\n",
    "            # print(W.shape)\n",
    "            # print('==============================')\n",
    "            # W_ls = []\n",
    "            # for i in range(X.shape[1]):\n",
    "            #     Wtemp = W[:,i]\n",
    "            #     Wtemp = Wtemp[:,None]\n",
    "            #     Xtemp = X[:,i]\n",
    "            #     Xtemp = Xtemp[:,None]\n",
    "            #     W_column = set_up_and_optimize_qubo(Xtemp, Y,Wtemp)\n",
    "            #     W_ls.append(W_column)\n",
    "            # W_optimized = np.stack(W_ls)\n",
    "            W_optimized = set_up_and_optimize_qubo(X, Y, W)\n",
    "            with torch.no_grad():\n",
    "                W.copy_( W.data - torch.from_numpy( args.learning_rate*W_optimized ) )\n",
    "                tempt = grad_compare[n_l]\n",
    "                tempt = torch.abs(tempt)\n",
    "                tempt[tempt < 0.01] = 0\n",
    "                grad_compare[n_l][tempt == 0] = 0\n",
    "                t = -torch.sign(grad_compare[n_l]).flatten()\n",
    "    \n",
    "                # t[t == 0] = 1\n",
    "                temp = torch.eq(t[t != 0], torch.from_numpy(W_optimized).flatten()[t != 0]).sum()\n",
    "                # if temp.item() == 150:\n",
    "                #     print(torch.sign(W.grad))\n",
    "                #     print(W_optimized)\n",
    "                #     print('-------------')\n",
    "                ls.append([temp.item(), (t != 0).sum().item()])\n",
    "                \n",
    "        ############ code from BNN_pytorch_new/tmp_2.py\n",
    "        elif update_type.split('.')[0] == 'old_qubo':\n",
    "            X = model.fc_list[n_l].input_features      \n",
    "            Y = - model.fc_list[n_l].output_features.grad\n",
    "            #W = model.fc_list[n_l].weight\n",
    "\n",
    "            BNN_on_QPU_libraries.x_data_reduced  = (X.data.numpy() +1.)/2. ## input in binary\n",
    "            BNN_on_QPU_libraries.selected_to_train = X.shape[0]   ### \"batch size\"\n",
    "            BNN_on_QPU_libraries.input_size = X.shape[1]\n",
    "            BNN_on_QPU_libraries.hidden_n = np.nan               ###\n",
    "            BNN_on_QPU_libraries.n_layers = 1               ### num layers\n",
    "            BNN_on_QPU_libraries.num_batches_to_run =  1   ### number of batches to run\n",
    "\n",
    "            BNN_on_QPU_libraries.model = BNN_on_QPU_libraries.BinaryNet()\n",
    "\n",
    "            for n_output  in range(  Y.shape[1] ):\n",
    "\n",
    "                #print('############### #################### ################')\n",
    "\n",
    "                BNN_on_QPU_libraries.y  = (Y.numpy()[:,n_output] +1. )/2. ## output in binary between zero and one\n",
    "\n",
    "                #BNN_on_QPU_libraries.output_dimension = Y.shape[1]\n",
    "\n",
    "\n",
    "                #print(BNN_on_QPU_libraries.model)\n",
    "                #BNN_on_QPU_libraries.optimization_type = 'grb_hard_c'\n",
    "                BNN_on_QPU_libraries.optimization_type = update_type.split('.')[1]\n",
    "                #BNN_on_QPU_libraries.optimization_type = 'sym_anneal_soft_qubo'\n",
    "                BNN_on_QPU_libraries.train_models(layers_to_optimize=[0])\n",
    "                BNN_on_QPU_libraries.list_results_grb_hard_c = BNN_on_QPU_libraries.list_results\n",
    "\n",
    "                layer_i = 0\n",
    "                _y = 0\n",
    "                for _x in range(X.shape[1]):\n",
    "                    W_solution = BNN_on_QPU_libraries.H_solution[f'weight_{layer_i}_{_y}_{_x}']\n",
    "                    W_solution = W_solution*2-1 #convert to spin\n",
    "                    W.grad[n_output,_x] = W_solution\n",
    "\n",
    "                W.data += args.learning_rate*W.grad\n",
    "\n",
    "        ############\n",
    "                \n",
    "        \n",
    "                \n",
    "        elif update_type == 'compare':\n",
    "            with torch.no_grad():\n",
    "                #W.copy_( W.data -args.learning_rate*model.fc_list[n_l].weight.grad )\n",
    "                W.copy_( W.data -args.learning_rate*W.grad )\n",
    "            X = model.fc_list[n_l].input_features.data      \n",
    "            Y = - model.fc_list[n_l].output_features.grad\n",
    "            plt.imshow( W.grad.numpy() )\n",
    "            plt.show()\n",
    "            W_optimized = set_up_and_optimize_qubo(X, Y, W)\n",
    "            plt.imshow( W_optimized )   \n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            plt.show()\n",
    "\n",
    "        elif update_type == 'compare_gt_grad':\n",
    "            with torch.no_grad():\n",
    "                #W.copy_( W.data -args.learning_rate*model.fc_list[n_l].weight.grad )\n",
    "                W.copy_( W.data -args.learning_rate*W.grad )\n",
    "            X = model.fc_list[n_l].input_features.data      \n",
    "            Y = - model.fc_list[n_l].output_features.grad\n",
    "            #plt.imshow( W.grad.numpy() )\n",
    "            #plt.show()\n",
    "            W_optimized = set_up_and_optimize_qubo(X, Y, W)\n",
    "            plt.imshow( model.fc_list[n_l].output_features.data.T )\n",
    "            plt.title( 'Y_data' )\n",
    "            plt.show()\n",
    "            plt.imshow( Y.T )\n",
    "            plt.title('Y')\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            plt.show()\n",
    "        return ls\n",
    "        return mean(ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_to_matrix(x):\n",
    "    x_data = np.zeros((len(x),123))\n",
    "    exists = np.zeros(123)\n",
    "    for i,x_i in enumerate( x ):\n",
    "        for j in range(123):\n",
    "            if j in x_i.keys():\n",
    "                assert x_i[j] == 1\n",
    "                x_data[i,j] = 1.\n",
    "                exists[j] = 1.\n",
    "    return x_data, exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    #m = math.floor(s / 60)\n",
    "    #s -= m * 60\n",
    "    return s\n",
    "\n",
    "# BNN code adapted from:\n",
    "# github.com/Akashmathwani/Binarized-Neural-networks-using-pytorch\n",
    "\n",
    "def test_loss():\n",
    "    global logs\n",
    "    data = x_test_data_reduced\n",
    "    target = y_test\n",
    "    data = data.astype('float32')\n",
    "    target = 1.*target\n",
    "    data = torch.from_numpy(data)\n",
    "    target = torch.from_numpy(target)\n",
    "    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "    #optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    output = model(data)\n",
    "    if num_classes == 2:\n",
    "        output = output.T[0]\n",
    "    loss = criterion(output, target)\n",
    "    # acc = metrics.roc_auc_score(output.detach().numpy(), target.detach().numpy())\n",
    "    # print(acc)\n",
    "    #return float(loss.data.detach().numpy())\n",
    "    logs['testloss'].append(float(loss.data.detach().numpy()))\n",
    "\n",
    "def train_nn(update_type='sgd', test_every_step=False, run_id = 0):\n",
    "    global logs\n",
    "    #if epoch == 1:\n",
    "    #    from IPython import embed; embed()\n",
    "    data_grad = x_data_reduced\n",
    "    data_grad = data_grad.astype('float32')\n",
    "    data_grad = torch.from_numpy(data_grad)\n",
    "    data_grad = data_grad*2 -1 ## convert to spin\n",
    "    target_grad = y\n",
    "    target_grad = torch.from_numpy(target_grad)\n",
    "\n",
    "    target_grad = 1.*target_grad\n",
    "    target_grad, target_grad = torch.autograd.Variable(target_grad), torch.autograd.Variable(target_grad)\n",
    "    for batch_n in range(num_batches_to_run):\n",
    "        \n",
    "        #if GRADIENT_DESCENT:\n",
    "        #    batch_n = 0\n",
    "            \n",
    "        #print(batch_n)\n",
    "\n",
    "        #print([i for i in  model.parameters()][0].grad)\n",
    "\n",
    "        #for index_i in range(selected_to_train):\n",
    "        #    in_x = x_data_reduced[batch_n*selected_to_train+index_i]\n",
    "        #    #in_x = 0+(in_x > 0.)\n",
    "        #    ( in_x, y[batch_n*selected_to_train+index_i])\n",
    "        data = x_data_reduced[batch_n*selected_to_train: (batch_n+1)*selected_to_train]\n",
    "        target = y[batch_n*selected_to_train: (batch_n+1)*selected_to_train]\n",
    "        #from IPython import embed; embed()\n",
    "\n",
    "        #if args.cuda:\n",
    "        #    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data = data.astype('float32')\n",
    "        \n",
    "        data = data*2 -1 ## convert to spin\n",
    "\n",
    "        #target = (target+1)/2. ## convert to spins\n",
    "        #target = (target*2.) -1. ## convert to spins\n",
    "        target = 1.*target\n",
    "        \n",
    "        if update_type == 'compare_gt_grad':\n",
    "            plt.imshow(np.array([target]))\n",
    "            plt.show()\n",
    "\n",
    "        data = torch.from_numpy(data)\n",
    "        target = torch.from_numpy(target)\n",
    "\n",
    "        data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        output_grad, _ = model(data_grad, return_list=['all'])\n",
    "        if num_classes == 2:\n",
    "            output_grad = output_grad.T[0]\n",
    "\n",
    "        #from IPython import embed; embed()\n",
    "        loss = criterion(output_grad, target_grad)\n",
    "        loss.backward()\n",
    "        W_grad = []\n",
    "        W_grad.append(model.fc_list[0].weight.grad)\n",
    "        # W_grad.append(model.fc_list[1].weight.grad)\n",
    "\n",
    "\n",
    "        model.zero_grad(set_to_none = True)\n",
    "        #output = model(data)\n",
    "        output, x_list = model(data, return_list=['all'])\n",
    "\n",
    "        ## output is a spin\n",
    "\n",
    "        #if epoch == 1 and batch_n == 0:\n",
    "        #    from IPython import embed; embed()\n",
    "\n",
    "        if num_classes == 2:\n",
    "            output = output.T[0]\n",
    "\n",
    "        #from IPython import embed; embed()\n",
    "        loss = criterion(output, target)\n",
    "        # acc = metrics.roc_auc_score(output.detach().numpy(), target.detach().numpy())\n",
    "        # print(acc)\n",
    "        #if epoch == 9 and batch_n == 0:\n",
    "        #    from IPython import embed; embed()\n",
    "        #from IPython import embed; embed()\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #for p in list(model.parameters()):\n",
    "        #    if hasattr(p,'org'):\n",
    "        #        print('ORG')\n",
    "        #        print(p)\n",
    "        #        p.data.copy_(p.org)\n",
    "        \n",
    "\n",
    "        diff = update(model, update_type = update_type, grad_compare = W_grad)\n",
    "        # logs['diff'].append(diff[0][0])\n",
    "        # logs['diff_sum'].append(diff[0][1])\n",
    "\n",
    "        #optimizer.step()\n",
    "        \n",
    "        #for p in list(model.parameters()):\n",
    "        #    if hasattr(p,'org'):\n",
    "        #        print('ORG')\n",
    "        #        print(p)\n",
    "        #        p.org.copy_(p.data.clamp_(-1,1))\n",
    "        \n",
    "        #from IPython import embed; embed()\n",
    "        #print(batch_n,loss.data.numpy())\n",
    "        #plt.plot([batch_n], [loss.data.numpy()], '.')\n",
    "        \n",
    "        #if epoch == 9 and batch_n == 0:\n",
    "        #    from IPython import embed; embed()\n",
    "\n",
    "        #if batch_idx % args.log_interval == 0:\n",
    "        if batch_n % args.log_interval == 0:\n",
    "            logs['tlos'].append(float(loss.data.detach().numpy()))\n",
    "            # with open(f\"/home/mkrahn/quantum_g/quantum_gnn/to_tolga_incremental_training_BNN/loss_{run_id}.csv\", \"a\") as f:\n",
    "            #     f.write(loss.data.detach().numpy())\n",
    "            #     f.write(f\"\\n\")\n",
    "            if test_every_step:\n",
    "                test_loss()\n",
    "        #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #        100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "#def adjust_learning_rate(optimizer, epoch):\n",
    "def adjust_learning_rate(epoch):\n",
    "    update_list = [55, 100, 150,200,400,600]\n",
    "    if epoch in update_list:\n",
    "        #for param_group in optimizer.param_groups:\n",
    "        #    param_group['lr'] = param_group['lr'] * 0.1\n",
    "        args.learning_rate = args.learning_rate *0.1\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def Train(update_type='sgd', test_every_step=False, run_id=0):\n",
    "    global logs\n",
    "    start = time.time()\n",
    "    logs = {}\n",
    "    logs['time_graph'] = []\n",
    "    logs['e']=[]\n",
    "    logs['accur']=[]\n",
    "    logs['tlos']=[]\n",
    "    logs['testloss']=[]\n",
    "    logs['lr'] = []\n",
    "    logs['diff'] = []\n",
    "    logs['diff_sum'] = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        #print('epoch: ', epoch)\n",
    "        #adjust_learning_rate(optimizer, epoch)\n",
    "        adjust_learning_rate(epoch)    \n",
    "        logs['e'].append(epoch)\n",
    "        if not test_every_step:\n",
    "            test_loss() \n",
    "        train_nn(update_type=update_type, test_every_step=test_every_step, run_id= run_id)   \n",
    "        seco=timeSince(start)\n",
    "        logs['time_graph'].append(seco)\n",
    "        #logs['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        logs['lr'].append(args.learning_rate)\n",
    "        #test()\n",
    "\n",
    "def plots():\n",
    "    #print(time_graph)\n",
    "    plt.title('Training with epoch', fontsize=20)\n",
    "    plt.ylabel('time (s)')\n",
    "    plt.plot(logs['e'],logs['time_graph'])\n",
    "    plt.show()\n",
    "    #plt.title('Accuracy With epoch', fontsize=20)\n",
    "    #plt.plot(logs['e'],logs['accur'])\n",
    "    #plt.show()\n",
    "    plt.title('Train loss', fontsize=20)\n",
    "    plt.plot(logs['tlos'])\n",
    "    plt.show()\n",
    "    plt.title('Test loss', fontsize=20)\n",
    "    plt.plot(logs['testloss'])\n",
    "    plt.show()\n",
    "    plt.title('lr', fontsize=20)\n",
    "    plt.plot(logs['lr'])\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdaa64d6cb19d87951a8f0febadec5607342f2236b34b1d2e6288f7bb60a4575"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
